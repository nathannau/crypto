{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install packaging\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "import time\n",
    "\n",
    "assert(version.parse(tf.__version__) >= version.parse(\"2.0.0-aplha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_serie(size, step) :\n",
    "    maxT = size*step\n",
    "    t = np.arange(0, maxT, step)\n",
    "#    return (np.sin(t+np.cos(t*0.31))).astype(np.float32)\n",
    "    return ((np.sin(t+np.sin(t*.31)) * np.sin(t /10 / 3.3485)).astype(np.float32)+1)*3\n",
    "#    return (np.sin(t+np.cos(t)) + .34158*np.sin(t * 3.3485)).astype(np.float32)\n",
    "#return (t * np.sin(t)/maxT + .3*np.sin(t * 3.3)).astype(np.float32)\n",
    "\n",
    "def split(arr, *count) :\n",
    "    total = sum(count)\n",
    "    p0 = 0\n",
    "    for i in count :\n",
    "        p1 = p0 + i\n",
    "        yield arr[int(len(arr)*p0/total):int(len(arr)*p1/total)]\n",
    "        p0 = p1\n",
    "    \n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "SEQ_LEN = 10\n",
    "N_STEPS = 50\n",
    "data = generate_serie(14001, .18)\n",
    "data = [(data[i:i+N_STEPS], data[i+N_STEPS:i+N_STEPS+SEQ_LEN]) for i in range(len(data)-N_STEPS-SEQ_LEN)] \n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "#plt.plot(data[1][0])\n",
    "#plt.plot(data[2][0])\n",
    "#plt.plot(data[3][0])\n",
    "#plt.show()\n",
    "\n",
    "spt_data = np.reshape([data[i][0] for i in range(len(data))], (-1, N_STEPS, 1))\n",
    "spt_lbl = np.reshape([data[i][1] for i in range(len(data))], (-1, SEQ_LEN, 1))\n",
    "\n",
    "train_size = 70\n",
    "valid_size = 20\n",
    "test_size = 10\n",
    "\n",
    "[train, valid, test] = zip(split(spt_data, train_size, valid_size, test_size), \n",
    "                           split(spt_lbl, train_size, valid_size, test_size))\n",
    "\n",
    "plt.plot(range(N_STEPS), train[0][1])\n",
    "plt.plot(range(N_STEPS, N_STEPS+SEQ_LEN), train[1][1])\n",
    "plt.plot(range(N_STEPS), train[0][2])\n",
    "plt.plot(range(N_STEPS, N_STEPS+SEQ_LEN), train[1][2])\n",
    "plt.plot(range(N_STEPS), train[0][3])\n",
    "plt.plot(range(N_STEPS, N_STEPS+SEQ_LEN), train[1][3])\n",
    "plt.show()\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train[0], train[1]))\n",
    "valid = tf.data.Dataset.from_tensor_slices((valid[0], valid[1]))\n",
    "test = tf.data.Dataset.from_tensor_slices((test[0], test[1]))\n",
    "#train = train.batch(64, True)\n",
    "#valid = valid.batch(64, True)\n",
    "#test = test.batch(64, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, hidden1, enc_units, batch_sz, seq_len) : #, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden1 = hidden1\n",
    "    self.enc_units = enc_units\n",
    "    self.batch_sz = batch_sz\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    self.encoder = tf.keras.layers.LSTM(self.hidden1,\n",
    "                                    #return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    stateful= not True,\n",
    "                                    #activation=\"relu\",\n",
    "                                    #activation=\"selu\",\n",
    "                                    #activation=\"linear\",\n",
    "                                    #recurrent_activation=\"selu\",  \n",
    "                                       )#recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    self.decoder = tf.keras.layers.LSTM(self.hidden1,\n",
    "                                    return_sequences=True,\n",
    "                                    #return_state=True,\n",
    "                                    stateful=not True,\n",
    "                                    #activation=\"relu\",\n",
    "                                    #activation=\"selu\",\n",
    "                                    #activation=\"linear\",\n",
    "                                    #recurrent_activation=\"selu\",  \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.dense = tf.keras.layers.Dense(self.enc_units,\n",
    "                                    activation=\"linear\")\n",
    "\n",
    "    \n",
    "#  def call(self, x, initial_state=None): #, hidden):\n",
    "  def call(self, x) :\n",
    "    mean = tf.keras.backend.mean(x, [1], True)\n",
    "    var = tf.keras.backend.var(x, [1], True)\n",
    "    x = (x - mean) / tf.keras.backend.sqrt(var)\n",
    "    \n",
    "    #return x\n",
    "#    [_, state, _] = self.encoder(x)\n",
    "    #[o, state1, state2] = self.encoder(x)\n",
    "    output = self.encoder(x)\n",
    "    state = output[1:]\n",
    "    #toto = self.encoder(x)\n",
    "#    print(\"++++++++++++++\")\n",
    "#    print(o)\n",
    "#    print(\"--------------\")\n",
    "#    print(state1)\n",
    "#    print(\"--------------\")\n",
    "#    print(state2)\n",
    "#    print(\"--------------\")\n",
    "    #print(o)\n",
    "    #print(\"--------------\")\n",
    "    #print(state)\n",
    "    #print(\"--------------\")\n",
    "    #print(o2)\n",
    "#    print(self.encoder.states)\n",
    "#    print(\"--------------\")\n",
    "    #print(\"--------------\")\n",
    "    #print(self.encoder.states)\n",
    "    \n",
    "#    state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(state1, state2)\n",
    "#    print(state)\n",
    "#    print(\"--------------\")\n",
    "    \n",
    "    d = tf.zeros([self.batch_sz, self.seq_len, self.enc_units])\n",
    "#    d = tf.zeros([1, 2, 3])\n",
    "    x = self.decoder(d, initial_state=state)\n",
    "#    x = self.decoder(d, initial_state=state2)\n",
    "#    x = self.decoder(d, initial_state=[state1, state2])\n",
    "#    x = self.decoder(d, initial_state=self.encoder.states)\n",
    "    x = self.dense(x)\n",
    "    x = x * tf.keras.backend.sqrt(var) + mean\n",
    "    return x\n",
    "\n",
    "#  def reset_states(self) :\n",
    "#    self.lstm.reset_states()\n",
    "\n",
    "#  def reset_states(self, initial_state=None) :\n",
    "#    #print(self.lstm.input_spec)\n",
    "#    self.lstm.reset_states(initial_state)\n",
    "\n",
    "#  def initialize_hidden_state(self):\n",
    "#    return tf.zeros((self.batch_sz, self.enc_units))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "encoder = Encoder(30, 1, BATCH_SIZE, SEQ_LEN)\n",
    "#encoder.reset_states(tf.compat.v1.nn.rnn_cell.LSTMStateTuple(8,30))\n",
    "#a = encoder.encoder.get_initial_state(tf.keras.Input(shape=(50, 1)))\n",
    "#a = encoder.encoder.get_initial_state(list(test.batch(BATCH_SIZE, True))[0])\n",
    "#print(a)\n",
    "encoder.build(tf.TensorShape([BATCH_SIZE, None, 1]))\n",
    "encoder.summary()\n",
    "encoder.compile(optimizer=\"Adam\", loss=\"MSE\", metrics=['accuracy'])\n",
    "encoder(tf.keras.Input(shape=(50, 1)))\n",
    "print(\"stateful\", encoder.stateful, encoder.state_updates)\n",
    "print(\"encoder\", encoder.input_shape, encoder.output_shape)\n",
    "#print(\"lstm\", encoder.lstm.input_shape, encoder.lstm.output_shape)\n",
    "#print(\"dense\", encoder.dense.input_shape, encoder.dense.output_shape)\n",
    "\n",
    "val = encoder.predict(test.batch(BATCH_SIZE, True))\n",
    "\n",
    "plt.figure(figsize=(15.0,10.0))\n",
    "plt.plot(np.zeros(50), color='k')\n",
    "\n",
    "colors = ['b','g','r']\n",
    "for i in range(0,3) :\n",
    "    plt.plot(list(test)[i][0], label=\"s0_\" + str(i), linestyle=':', color=colors[i])\n",
    "#    plt.plot(list(test)[i][1], label=\"s1_\" + str(i))\n",
    "    plt.plot(val[i], label=\"p_\" + str(i), linestyle='-', color=colors[i])\n",
    "#    plt.plot(val0[i], label=\"p_\" + str(i), linestyle='-')\n",
    "#    plt.plot(val1[i], label=\"p_\" + str(i), linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #*8\n",
    "\n",
    "#encoder = Encoder(10, N_STEPS, BATCH_SIZE)\n",
    "encoder = Encoder(10, 1, BATCH_SIZE, SEQ_LEN)\n",
    "encoder.build(tf.TensorShape([BATCH_SIZE, None, 1]))\n",
    "encoder.summary()\n",
    "encoder.compile(optimizer=\"Adam\", loss=\"MSE\", metrics=['accuracy'])\n",
    "encoder(tf.keras.Input(shape=(50, 1)))\n",
    "print(\"stateful\", encoder.stateful, encoder.state_updates)\n",
    "print(\"encoder\", encoder.input_shape, encoder.output_shape)\n",
    "#print(\"lstm\", encoder.lstm.input_shape, encoder.lstm.output_shape)\n",
    "#print(\"dense\", encoder.dense.input_shape, encoder.dense.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 12*5\n",
    "EPOCHS = 50 #8*40\n",
    "\n",
    "#encoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "#print(len(train[0]))\n",
    "#print(len(train[0])//BATCH_SIZE)\n",
    "#print(len(train[0])//BATCH_SIZE*BATCH_SIZE)\n",
    "#print(len(valid[0]))\n",
    "#print(len(valid[0])//BATCH_SIZE)\n",
    "#print(len(valid[0])//BATCH_SIZE*BATCH_SIZE)\n",
    "#tbs = len(train[0])//BATCH_SIZE*BATCH_SIZE\n",
    "#vbs = len(valid[0])//BATCH_SIZE*BATCH_SIZE\n",
    "\n",
    "#a = encoder(train[0][:8])\n",
    "#print(a)\n",
    "#history = encoder.fit(train, epochs=EPOCHS) #, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#history = encoder.fit(train[0][:tbs], train[1][:tbs], epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "history = encoder.fit(train.batch(BATCH_SIZE, True), epochs=EPOCHS, validation_data=valid.batch(BATCH_SIZE, True), shuffle=False) # , batch_size=BATCH_SIZE\n",
    "#history = encoder.fit(train.batch(BATCH_SIZE, True), epochs=EPOCHS, validation_data=valid.batch(BATCH_SIZE, True), callbacks=[ResetCallback()], shuffle=False) # , batch_size=BATCH_SIZE\n",
    "#history = encoder.fit(train.batch(64, True), epochs=EPOCHS, validation_data=valid.batch(64, True), callbacks=[ResetCallback()], shuffle=False) # , batch_size=BATCH_SIZE\n",
    "#history = encoder.fit(train[0][:tbs], train[1][:tbs], epochs=EPOCHS, validation_data=(valid[0][:vbs], valid[1][:vbs]), batch_size=BATCH_SIZE)\n",
    "#a = encoder(train[0][:8])\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history)\n",
    "#print(history.history)\n",
    "print(history.history.keys())\n",
    "of = 9\n",
    "plt.figure(figsize=(15.0,10.0))\n",
    "plt.plot(history.history['loss'][of:], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'][of:], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(history.history['accuracy'][of:], label=\"accuracy\")\n",
    "plt.plot(history.history['val_accuracy'][of:], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = encoder.predict(test.batch(BATCH_SIZE, True))\n",
    "#val = encoder.predict(val)\n",
    "#val = encoder.predict(val)\n",
    "#val = encoder.predict(val)\n",
    "#val = encoder.predict(val)\n",
    "#val = encoder.predict(val)\n",
    "\n",
    "plt.figure(figsize=(15.0,10.0))\n",
    "for i in range(0,6) :\n",
    "    plt.plot(range(N_STEPS), list(test)[i][0], label=\"s0_\" + str(i), linestyle=':')\n",
    "    plt.plot(range(N_STEPS, N_STEPS+SEQ_LEN), list(test)[i][1], label=\"s1_\" + str(i))\n",
    "    plt.plot(range(N_STEPS, N_STEPS+SEQ_LEN), val[i], label=\"p_\" + str(i), linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
