{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install packaging\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "import time\n",
    "\n",
    "assert(version.parse(tf.__version__) >= version.parse(\"2.0.0-aplha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_serie(size, step) :\n",
    "    maxT = size*step\n",
    "    t = np.arange(0, maxT, step)\n",
    "#    return (np.sin(t+np.cos(t*0.31))).astype(np.float32)\n",
    "    return (np.sin(t+np.sin(t*.31)) * np.sin(t / 3.3485)).astype(np.float32)\n",
    "#    return (np.sin(t+np.cos(t)) + .34158*np.sin(t * 3.3485)).astype(np.float32)\n",
    "#return (t * np.sin(t)/maxT + .3*np.sin(t * 3.3)).astype(np.float32)\n",
    "\n",
    "def split(arr, *count) :\n",
    "    total = sum(count)\n",
    "    p0 = 0\n",
    "    for i in count :\n",
    "        p1 = p0 + i\n",
    "        yield arr[int(len(arr)*p0/total):int(len(arr)*p1/total)]\n",
    "        p0 = p1\n",
    "    \n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "N_STEPS = n_steps = 50\n",
    "data = generate_serie(14001, .18)\n",
    "data = [(data[i:i+n_steps], data[i+1:i+n_steps+1]) for i in range(len(data)-n_steps-1)] \n",
    "np.random.shuffle(data)\n",
    "\n",
    "#plt.plot(data[1][0])\n",
    "#plt.plot(data[2][0])\n",
    "#plt.plot(data[3][0])\n",
    "#plt.show()\n",
    "\n",
    "spt_data = np.reshape([data[i][0] for i in range(len(data))], (-1, n_steps, 1))\n",
    "spt_lbl = np.reshape([data[i][1] for i in range(len(data))], (-1, n_steps, 1))\n",
    "\n",
    "train_size = 80\n",
    "valid_size = 15\n",
    "test_size = 5\n",
    "\n",
    "[train, valid, test] = zip(split(spt_data, train_size, valid_size, test_size), \n",
    "                           split(spt_lbl, train_size, valid_size, test_size))\n",
    "\n",
    "plt.plot(train[0][1])\n",
    "plt.plot(train[0][2])\n",
    "plt.plot(train[0][3])\n",
    "plt.show()\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train[0], train[1]))\n",
    "valid = tf.data.Dataset.from_tensor_slices((valid[0], valid[1]))\n",
    "test = tf.data.Dataset.from_tensor_slices((test[0], test[1]))\n",
    "#train = train.batch(64, True)\n",
    "#valid = valid.batch(64, True)\n",
    "#test = test.batch(64, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    #BATCH_SIZE = 64\n",
    "    BATCH_SIZE = 8\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((spt_data, spt_lbl))\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices(spt_data)\n",
    "    #dataset = dataset.batch(BATCH_SIZE, True)\n",
    "    #print(dataset.take(64))\n",
    "    #for i,(a,b) in enumerate(dataset) : print(\"------\", i, a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, hidden1, enc_units, batch_sz) : #, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden1 = hidden1\n",
    "    self.enc_units = enc_units\n",
    "    self.batch_sz = batch_sz\n",
    "    #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    #self.inputi = tf.keras.Input(shape=(1,))\n",
    "    #self.input_shape = tf.TensorShape([None,1])\n",
    "    self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                    return_sequences=True,\n",
    "                                    #return_state=True,\n",
    "                         #           stateful=True,\n",
    "                                    #activation=\"selu\",\n",
    "                                    #activation=\"linear\",\n",
    "                                    #recurrent_activation=\"selu\",  \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.dense = tf.keras.layers.Dense(self.enc_units,\n",
    "                                    activation=\"linear\")\n",
    "\n",
    "    #self.gru = tf.keras.layers.RNN(\n",
    "    #    tf.keras.layers.GRUCell(self.enc_units,\n",
    "    #                               recurrent_initializer='glorot_uniform'),\n",
    "    #   return_sequences=True,\n",
    "    #   return_state=True\n",
    "    #)\n",
    "    \n",
    "#  def call(self, x, initial_state=None): #, hidden):\n",
    "  def call(self, x) : #, initial_state=None): #, hidden):\n",
    "    #print(x, initial_state)\n",
    "    #x2 = self.inputi(x)\n",
    "    #print(x2)\n",
    "#    if (initial_state!=None) :\n",
    "#        self.lstm.reset_states(initial_state)\n",
    "    \n",
    "    #output = self.lstm(x) #, initial_state = hidden)\n",
    "    x = self.lstm(x) #, initial_state = hidden)\n",
    "    output = self.dense(x)\n",
    "    #output, self.state = self.lstm(x, self.state) #, initial_state = hidden)\n",
    "    return output #, state\n",
    "\n",
    "  #def reset_states(self) :\n",
    "  #  self.lstm.reset_states()\n",
    "\n",
    "#  def reset_states(self, initial_state=None) :\n",
    "#    #print(self.lstm.input_spec)\n",
    "#    self.lstm.reset_states(initial_state)\n",
    "\n",
    "#  def initialize_hidden_state(self):\n",
    "#    return tf.zeros((self.batch_sz, self.enc_units))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.MeanSquaredError()\n",
    "    mask = np.concatenate((\n",
    "        np.zeros((19,1), dtype=np.float32), \n",
    "        np.ones((1,1), dtype=np.float32)))\n",
    "    #print(mask)\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        #print(\"loss_function\")\n",
    "        #mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        #print(real, pred)\n",
    "        real *= mask\n",
    "        pred *= mask\n",
    "        #print(real, pred)\n",
    "        loss_ = loss_object(real, pred)\n",
    "        #print(loss_)\n",
    "\n",
    "        #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        #loss_ *= mask\n",
    "        #print(loss_)\n",
    "\n",
    "        return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    #@tf.function\n",
    "    def train_step(model, inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Make a prediction on all the batch\n",
    "            predictions = model(inputs)\n",
    "            # Get the error/loss on these predictions\n",
    "            loss = loss_function(targets, predictions)\n",
    "        # Compute the gradient which respect to the loss\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # Change the weights of the model\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "        #train_loss(loss)\n",
    "        #train_accuracy(targets, predictions)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64*8\n",
    "EPOCHS = 4\n",
    "\n",
    "#encoder = keras.models.Sequential([\n",
    "#    keras.layers.LSTM(20, return_sequences=True),\n",
    "#    keras.layers.LSTM(20, return_sequences=True),\n",
    "#    keras.layers.TimeDistributed(keras.layers.Dense(1))\n",
    "#])\n",
    "\n",
    "encoder = Encoder(10, N_STEPS, BATCH_SIZE)\n",
    "encoder.build(tf.TensorShape([BATCH_SIZE, None, 1]))\n",
    "encoder.summary()\n",
    "encoder.compile(optimizer=\"Adam\", loss=\"MSE\")\n",
    "#encoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "#print(len(train[0]))\n",
    "#print(len(train[0])//BATCH_SIZE)\n",
    "#print(len(train[0])//BATCH_SIZE*BATCH_SIZE)\n",
    "#print(len(valid[0]))\n",
    "#print(len(valid[0])//BATCH_SIZE)\n",
    "#print(len(valid[0])//BATCH_SIZE*BATCH_SIZE)\n",
    "#tbs = len(train[0])//BATCH_SIZE*BATCH_SIZE\n",
    "#vbs = len(valid[0])//BATCH_SIZE*BATCH_SIZE\n",
    "\n",
    "#a = encoder(train[0][:8])\n",
    "#print(a)\n",
    "#history = encoder.fit(train, epochs=EPOCHS) #, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#history = encoder.fit(train[0][:tbs], train[1][:tbs], epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "history = encoder.fit(train.batch(64, True), epochs=EPOCHS, validation_data=valid.batch(64, True), shuffle=False) # , batch_size=BATCH_SIZE\n",
    "#history = encoder.fit(train[0][:tbs], train[1][:tbs], epochs=EPOCHS, validation_data=(valid[0][:vbs], valid[1][:vbs]), batch_size=BATCH_SIZE)\n",
    "#a = encoder(train[0][:8])\n",
    "#print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(history)\n",
    "#print(history.history)\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    EPOCHS = 400\n",
    "\n",
    "    encoder = Encoder(10, 50, BATCH_SIZE)\n",
    "    encoder.build(tf.TensorShape([BATCH_SIZE, None, 1]))\n",
    "    encoder.summary()\n",
    "\n",
    "    #print(dataset)\n",
    "    #print(len(list(dataset)))\n",
    "    steps_per_epoch = len(list(dataset)) #//BATCH_SIZE\n",
    "    #print(steps_per_epoch)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "      start = time.time()\n",
    "\n",
    "      enc_hidden = encoder.initialize_hidden_state()\n",
    "      encoder.reset_states()\n",
    "      total_loss = 0\n",
    "\n",
    "      #print(dataset)\n",
    "      for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        #if (batch!=0) : continue\n",
    "        #print(len(inp), len(targ))\n",
    "        batch_loss = train_step(encoder, inp, targ)\n",
    "        #print(batch_loss)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "      print('Epoch {} Loss {:.4f}'.format(epoch + 1, \n",
    "                                          total_loss / steps_per_epoch))\n",
    "    #    break\n",
    "      print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    #  break\n",
    "      # saving (checkpoint) the model every 2 epochs\n",
    "    #  if (epoch + 1) % 2 == 0:\n",
    "    #    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    tm = np.ones((20,1), dtype=np.float32) - mask\n",
    "    d = list(dataset)[0]\n",
    "    #print(d[0])\n",
    "    t = encoder(d[0])\n",
    "    #print(t)\n",
    "    #print(\"-------\")\n",
    "    print(t- d[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    #BATCH_SIZE = 64\n",
    "    #f = dataset.take(1)\n",
    "    #print(f.shape)\n",
    "    f = next(iter(dataset))\n",
    "    #print(f)\n",
    "    print(\"f : \", f.shape)\n",
    "    #print(\"f : \", f.shape, f)\n",
    "    #f = tf.zeros((BATCH_SIZE, 2, 10))\n",
    "    #f = tf.ones((BATCH_SIZE, 1, 1))\n",
    "\n",
    "    #print(list(f))\n",
    "    #print(\"f : \", f.shape, f)\n",
    "    #print(\"f2 : \", f2.shape, f2)\n",
    "    encoder = Encoder(1, 3) # BATCH_SIZE)\n",
    "    #print(encoder.lstm.input_spec[0])\n",
    "    #encoder.reset_states(encoder.initialize_hidden_state());\n",
    "    encoder.build(tf.TensorShape([3,None,1]))\n",
    "\n",
    "    \"\"\" \n",
    "        shape = (A, B, C)\n",
    "        A : BATCH_SIZE (nombre de lot envoyer en meme temps)\n",
    "        B : Nombre de d√©pliage\n",
    "        C : Nombre d'input \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #encoder.build(tf.TensorShape([1,None,1]))\n",
    "    #encoder.build([tf.TensorShape([5,1,1]), tf.TensorShape([5,1])])\n",
    "    encoder.summary()\n",
    "    print(encoder.lstm.states)\n",
    "    #encoder.reset_states()\n",
    "    f = tf.constant([[[1.]],[[0.2]],[[0.5]]], dtype=tf.float32)\n",
    "    sample_output = encoder(f) #, sample_hidden)\n",
    "    print(\"---------------\")\n",
    "    print(sample_output)\n",
    "    print(\"---------------\")\n",
    "    print(encoder.lstm.states[1])\n",
    "\n",
    "    \"\"\"encoder.lstm.reset_states()\n",
    "    sample_output = encoder(f) #, sample_hidden)\n",
    "    print(\"---------------\")\n",
    "    print(sample_output)\n",
    "    print(\"---------------\")\n",
    "    print(encoder.lstm.states[1])\"\"\"\n",
    "\n",
    "    encoder.summary()\n",
    "\n",
    "    #sample_hidden = encoder.initialize_hidden_state()\n",
    "    #sample_hidden = tf.zeros((BATCH_SIZE,1))\n",
    "    #print(sample_hidden)\n",
    "    #sample_output, sample_hidden = encoder(f) #, sample_hidden)\n",
    "    #sample_output = encoder(f) #, sample_hidden)\n",
    "    #encoder.summary()\n",
    "\n",
    "    #print(encoder.gru.dynamic)\n",
    "    #print(\"---------------\")\n",
    "    #sample_output, sample_hidden = encoder(f2, sample_hidden)\n",
    "    #print(\"---------------\")\n",
    "    #print(sample_output)\n",
    "    #print(sample_output, sample_hidden)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
